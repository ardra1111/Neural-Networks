{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VrVl63Mcf6TC"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For data manipulation\n",
        "import numpy as np  # For numerical operations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're importing the libraries we need for our project. `pandas` helps us work with data tables, while `numpy` is great for numerical operations."
      ],
      "metadata": {
        "id": "FJzkGtIOrnRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with sample data\n",
        "data = pd.DataFrame(data={\"input1\": [0, 0, 1, 1], \"input2\": [0, 1, 0, 1], \"target\": [0, 1, 1, 0]})\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "mGrV8CDvhrOS",
        "outputId": "806f74c2-cf42-4b62-80b9-551f6dc76e37"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   input1  input2  target\n",
              "0       0       0       0\n",
              "1       0       1       1\n",
              "2       1       0       1\n",
              "3       1       1       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-be803d8c-67b9-41f6-8813-3883784d23a5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input1</th>\n",
              "      <th>input2</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be803d8c-67b9-41f6-8813-3883784d23a5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-be803d8c-67b9-41f6-8813-3883784d23a5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-be803d8c-67b9-41f6-8813-3883784d23a5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a0391484-562b-4003-8c96-18b1052f6dae\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a0391484-562b-4003-8c96-18b1052f6dae')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a0391484-562b-4003-8c96-18b1052f6dae button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"input1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're making a simple dataset to train our neural network. This dataset consists of four examples, each with two input features (`input1 and input2`) and one output (`target`). These examples represent the **XOR** gate function.\n"
      ],
      "metadata": {
        "id": "lXGaph4Lr1lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target variable into numpy arrays\n",
        "X = np.array(data.iloc[:, 0:2])  # Feature matrix\n",
        "y = np.array(data.iloc[:, 2])  # Target vector"
      ],
      "metadata": {
        "id": "fHOXa6q0hyAq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Converting Dataset into Arrays:**\n",
        "Now, we're converting our dataset into arrays to prepare them for training our neural network. `X` contains the input features, and `y` contains the corresponding outputs.\n"
      ],
      "metadata": {
        "id": "-0eT_JM8sQiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to initialize biases for a layer\n",
        "def initialize_biases(num_neurons):\n",
        "    return np.random.uniform(low=-1, high=1, size=(1, num_neurons))\n"
      ],
      "metadata": {
        "id": "aKfheNsrh76p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initializing Layer Biases:**\n",
        "Here, we're defining a function `initialize_biases` to randomly initialize biases for each neuron in a layer. Biases are like additional parameters that help the neural network learn.\n"
      ],
      "metadata": {
        "id": "6nbwgXAlsk2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to initialize weights for a layer\n",
        "def initialize_weights(num_inputs, num_neurons):\n",
        "    return np.random.uniform(low=-1, high=1, size=(num_inputs, num_neurons))\n"
      ],
      "metadata": {
        "id": "RTGQzJh2iFNk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initializing Layer Weights:**\n",
        "We're defining another function `initialize_weights` to randomly initialize weights for connections between neurons in two layers. Weights determine the strength of connections between neurons.\n"
      ],
      "metadata": {
        "id": "OsP02_i-s-x6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define neural network structure and activation functions\n",
        "NUM_INPUTS = X.shape[1]  # Number of input features\n",
        "NUM_LAYERS = 3  # Number of layers in the network (input + hidden + output)\n",
        "neurons_per_layer = [NUM_INPUTS]  # List to store the number of neurons per layer\n",
        "\n",
        "# Dictionary of activation functions\n",
        "activation_functions_dict = {0: \"linear\", 1: \"sigmoid\", 2: \"tanh\", 3: \"relu\", 4: \"leaky_relu\"}\n",
        "activation_functions = [None]  # List to store activation functions per layer\n",
        "\n",
        "# Initialize lists for weights and biases\n",
        "biases = [None]\n",
        "weights = [None]\n",
        "\n",
        "# Initialize forward pass variables\n",
        "layer_weighted_sums = [None] * NUM_LAYERS\n",
        "layer_outputs = [None] * NUM_LAYERS\n",
        "\n",
        "# Initialize backpropagation variables\n",
        "activation_derivatives = [None] * NUM_LAYERS\n",
        "bias_gradients = [None] * NUM_LAYERS\n",
        "weight_gradients = [None] * NUM_LAYERS\n",
        "loss_gradients = [None] * NUM_LAYERS\n"
      ],
      "metadata": {
        "id": "rEKvJHtyiI7p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining Neural Network Architecture:**\n",
        "We're setting up the architecture of our neural network. `NUM_INPUTS` represents the number of input features, `NUM_LAYERS` is the total number of layers in the network, and `neurons_per_layer` keeps track of the number of neurons in each layer.\n",
        "\n",
        "The input layer has 2 neurons, representing the features of the input data.\n",
        "\n",
        "The hidden layer (if present) has 2 neurons in the first cell, 3 neurons\n",
        "in the seventh cell, and 3 neurons in the thirteenth cell.\n",
        "\n",
        "The output layer has 1 neuron, which outputs the prediction of the neural network\n"
      ],
      "metadata": {
        "id": "kx_xt2TEtK4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure hidden and output layers\n",
        "for layer in range(1, NUM_LAYERS-1):\n",
        "    num_neurons = int(input(f\"How many neurons for Hidden Layer #{layer}? \"))\n",
        "    neurons_per_layer.append(num_neurons)\n",
        "\n",
        "    activation_func_index = int(input(f\"Choose activation function for Hidden Layer #{layer} (0: Linear, 1: Sigmoid, 2: Tanh, 3: ReLU, 4: Leaky ReLU): \"))\n",
        "    activation_functions.append(activation_functions_dict[activation_func_index])\n",
        "\n",
        "    biases.append(initialize_biases(num_neurons))\n",
        "    weights.append(initialize_weights(neurons_per_layer[layer-1], num_neurons))\n",
        "\n",
        "# Configure output layer based on the task\n",
        "task_type = int(input(\"Choose task type (0: Regression, 1: Classification): \"))\n",
        "\n",
        "if task_type == 0:\n",
        "    output_dimension = 1 if int(input(\"Regression output type (0: Scalar, 1: Vector): \")) == 0 else int(input(\"Number of output dimensions: \"))\n",
        "    activation_functions.append(\"linear\")\n",
        "else:\n",
        "    if int(input(\"Realize a boolean function (0: No, 1: Yes): \")) == 1:\n",
        "        output_dimension = 1\n",
        "        activation_functions.append(\"sigmoid\")\n",
        "    else:\n",
        "        output_dimension = int(input(\"Number of output classes (>2): \"))\n",
        "        activation_functions.append(\"softmax\")\n",
        "\n",
        "neurons_per_layer.append(output_dimension)\n",
        "biases.append(initialize_biases(output_dimension))\n",
        "weights.append(initialize_weights(neurons_per_layer[-2], output_dimension))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j89cUtVYiLn3",
        "outputId": "00b561bc-d665-42d4-cad8-f5f684ed8500"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How many neurons for Hidden Layer #1? 3\n",
            "Choose activation function for Hidden Layer #1 (0: Linear, 1: Sigmoid, 2: Tanh, 3: ReLU, 4: Leaky ReLU): 1\n",
            "Choose task type (0: Regression, 1: Classification): 1\n",
            "Realize a boolean function (0: No, 1: Yes): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Customizing Neural Network:**\n",
        "We ask the user for preferences to customize the neural network. It prompts the user to specify the number of neurons and the activation function for each hidden layer, as well as the task type (regression or classification).\n",
        "\n",
        "- **How many neurons for Hidden Layer #1?**: We choose to have 3 neurons in the first hidden layer.\n",
        "\n",
        "- **Choose activation function for Hidden Layer #1**: Here, we select the Sigmoid activation function (option 1) for the first hidden layer. Sigmoid is often suitable for binary classification tasks due to its output range between 0 and 1.\n",
        "\n",
        "- **Choose task type**: We specify that the task type is classification (option 1). This indicates that the neural network will be trained for a classification task.\n",
        "\n",
        "- **Realize a boolean function**: We opt to realize a boolean function (option 1), which aligns with our problem statement of binary classification for the XOR gate prediction.\n",
        "\n",
        "\n",
        "**Choice of Activation Function**\n",
        "\n",
        "In the context of realizing a boolean function like the **XOR gate** , using the **Sigmoid activation function is a common choice** due to its ability to produce outputs in the range of 0 to 1, suitable for binary classification tasks.The Sigmoid function **maps any input value to a value between 0 and 1**. which is perfect for problems where the output needs to be binary (0 or 1), as in the case of the XOR gate.\n",
        "\n",
        "It's worth noting that while Sigmoid is often preferred for boolean functions due to its interpretability and clear decision boundary, the choice of activation function can also depend on factors like network architecture, dataset complexity, and experimentation to find the best-performing model.\n"
      ],
      "metadata": {
        "id": "riYvosVUt5Hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute weighted sum for layer neurons\n",
        "def compute_weighted_sum(prev_layer_output, layer_biases, layer_weights):\n",
        "    return layer_biases + np.dot(prev_layer_output, layer_weights)\n"
      ],
      "metadata": {
        "id": "fSbtJccgiOm6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating Weighted Sums:** We're defining a function `compute_weighted_sum` to calculate the weighted sum of inputs to neurons in a layer. This sum is then used as input to activation functions.\n"
      ],
      "metadata": {
        "id": "-JsEFaKX4KN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute neuron outputs using activation functions\n",
        "def compute_layer_output(weighted_sums, activation_function):\n",
        "    if activation_function == \"linear\":\n",
        "        return weighted_sums\n",
        "    elif activation_function == \"sigmoid\":\n",
        "        return 1 / (1 + np.exp(-weighted_sums))\n",
        "    elif activation_function == \"tanh\":\n",
        "        return np.tanh(weighted_sums)\n",
        "    elif activation_function == \"relu\":\n",
        "        return np.maximum(0, weighted_sums)\n",
        "    elif activation_function == \"leaky_relu\":\n",
        "        return np.where(weighted_sums > 0, weighted_sums, 0.01 * weighted_sums)\n"
      ],
      "metadata": {
        "id": "0vVvnJs-ie-M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating Neuron Outputs:** Here, we're defining a function `compute_layer_output` to compute the output of neurons in a layer after applying an activation function. The activation function determines whether the neuron fires or not."
      ],
      "metadata": {
        "id": "fKWYq5vT4h1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute derivative of activation function\n",
        "def compute_activation_derivative(activation_function, weighted_sums):\n",
        "    if activation_function == \"linear\":\n",
        "        return np.ones_like(weighted_sums)\n",
        "    elif activation_function == \"sigmoid\":\n",
        "        outputs = 1 / (1 + np.exp(-weighted_sums))\n",
        "        return outputs * (1 - outputs)\n",
        "    elif activation_function == \"tanh\":\n",
        "        return 1 - np.tanh(weighted_sums)**2\n",
        "    elif activation_function == \"relu\":\n",
        "        return (weighted_sums > 0).astype(float)\n",
        "    elif activation_function == \"leaky_relu\":\n",
        "        return np.where(weighted_sums > 0, 1, 0.01)\n"
      ],
      "metadata": {
        "id": "7yDPelXjihuW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating Derivatives:** We're defining a function `compute_activation_derivative` to compute the derivative of neuron outputs with respect to their weighted sums. This derivative is needed during the backpropagation process.\n"
      ],
      "metadata": {
        "id": "Xi2dze3p5C0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute gradient with respect to biases\n",
        "def compute_bias_gradients(output_gradients):\n",
        "    return output_gradients\n"
      ],
      "metadata": {
        "id": "McVtKVl0inBM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now, we're defining a function `compute_bias_gradients` to calculate the derivative of neuron outputs with respect to their biases. Biases help control how much a neuron activates.\n"
      ],
      "metadata": {
        "id": "2cQzRM1s5a69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute gradient with respect to weights\n",
        "def compute_weight_gradients(prev_layer_output, output_gradients):\n",
        "    return np.dot(prev_layer_output.T, output_gradients)\n"
      ],
      "metadata": {
        "id": "IDYV3NTDirTU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Here, we're defining a function `compute_weight_gradients` to compute the derivative of neuron outputs with respect to their weights. These derivatives help update the weights during training.\n"
      ],
      "metadata": {
        "id": "n0yNWroy5pMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward and backward propagation for each training example\n",
        "for example_index in range(X.shape[0]):\n",
        "    layer_outputs[0] = X[example_index].reshape(1, X.shape[1])  # Input layer output\n",
        "\n",
        "    # Forward pass\n",
        "    for layer in range(1, NUM_LAYERS):\n",
        "        layer_weighted_sums[layer] = compute_weighted_sum(layer_outputs[layer-1], biases[layer], weights[layer])  # Compute weighted sum\n",
        "        layer_outputs[layer] = compute_layer_output(layer_weighted_sums[layer], activation_functions[layer])  # Compute neuron outputs\n",
        "\n",
        "        activation_derivatives[layer] = compute_activation_derivative(activation_functions[layer], layer_weighted_sums[layer])  # Compute activation function derivative\n",
        "        bias_gradients[layer] = compute_bias_gradients(activation_derivatives[layer])  # Gradient wrt biases\n",
        "        weight_gradients[layer] = compute_weight_gradients(layer_outputs[layer-1], activation_derivatives[layer])  # Gradient wrt weights\n",
        "\n",
        "    # Compute loss\n",
        "    y[example_index] = y[example_index].reshape(1, 1)\n",
        "    loss = (1/2) * (y[example_index] - layer_outputs[NUM_LAYERS-1])**2  # Squared error loss\n",
        "\n",
        "    # Backward pass\n",
        "    loss_gradients[NUM_LAYERS-1] = layer_outputs[NUM_LAYERS-1] - y[example_index]  # Gradient of loss wrt output\n",
        "    for layer in range(NUM_LAYERS-2, 0, -1):\n",
        "        loss_gradients[layer] = np.dot(loss_gradients[layer+1], weights[layer+1].T) * activation_derivatives[layer]  # Gradient of loss wrt hidden layers\n",
        "\n",
        "    # Print shapes for debugging\n",
        "    for layer in range(1, NUM_LAYERS):\n",
        "        print(f\"Layer {layer} shapes: z[{layer}].shape = {layer_weighted_sums[layer].shape}, h[{layer}].shape = {layer_outputs[layer].shape}, del_hl_by_del_theta0[{layer}].shape = {bias_gradients[layer].shape}, del_hl_by_del_theta[{layer}].shape = {weight_gradients[layer].shape}\")\n",
        "\n",
        "    print(f\"Loss shape for example {example_index}: {loss.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vpDY3ydiu1h",
        "outputId": "2f775c92-2d07-4c8b-8c0c-878d392eb3c7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1 shapes: z[1].shape = (1, 3), h[1].shape = (1, 3), del_hl_by_del_theta0[1].shape = (1, 3), del_hl_by_del_theta[1].shape = (2, 3)\n",
            "Layer 2 shapes: z[2].shape = (1, 1), h[2].shape = (1, 1), del_hl_by_del_theta0[2].shape = (1, 1), del_hl_by_del_theta[2].shape = (3, 1)\n",
            "Loss shape for example 0: (1, 1)\n",
            "Layer 1 shapes: z[1].shape = (1, 3), h[1].shape = (1, 3), del_hl_by_del_theta0[1].shape = (1, 3), del_hl_by_del_theta[1].shape = (2, 3)\n",
            "Layer 2 shapes: z[2].shape = (1, 1), h[2].shape = (1, 1), del_hl_by_del_theta0[2].shape = (1, 1), del_hl_by_del_theta[2].shape = (3, 1)\n",
            "Loss shape for example 1: (1, 1)\n",
            "Layer 1 shapes: z[1].shape = (1, 3), h[1].shape = (1, 3), del_hl_by_del_theta0[1].shape = (1, 3), del_hl_by_del_theta[1].shape = (2, 3)\n",
            "Layer 2 shapes: z[2].shape = (1, 1), h[2].shape = (1, 1), del_hl_by_del_theta0[2].shape = (1, 1), del_hl_by_del_theta[2].shape = (3, 1)\n",
            "Loss shape for example 2: (1, 1)\n",
            "Layer 1 shapes: z[1].shape = (1, 3), h[1].shape = (1, 3), del_hl_by_del_theta0[1].shape = (1, 3), del_hl_by_del_theta[1].shape = (2, 3)\n",
            "Layer 2 shapes: z[2].shape = (1, 1), h[2].shape = (1, 1), del_hl_by_del_theta0[2].shape = (1, 1), del_hl_by_del_theta[2].shape = (3, 1)\n",
            "Loss shape for example 3: (1, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-79353957656f>:15: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  y[example_index] = y[example_index].reshape(1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward and Backward Passes:** performing the forward and backward passes of the neural network for each training example. It calculates the output of each layer, computes the loss, and updates the gradients during backpropagation.\n"
      ],
      "metadata": {
        "id": "Yxuntag95_x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient descent algorithm for training the neural network\n",
        "LEARNING_RATE = 0.01  # Learning rate for gradient descent\n",
        "TOLERANCE = 1e-6  # Tolerance for stopping criterion\n",
        "MAX_EPOCHS = 10000  # Maximum number of epochs to prevent infinite loop\n",
        "\n",
        "previous_loss = np.inf  # Initialize previous loss to infinity\n",
        "\n",
        "for epoch in range(MAX_EPOCHS):\n",
        "    total_loss = 0  # Initialize total loss for the epoch\n",
        "\n",
        "    for example_index in range(X.shape[0]):\n",
        "        layer_outputs[0] = X[example_index].reshape(1, X.shape[1])  # Set input layer output\n",
        "\n",
        "        # Forward pass\n",
        "        for layer in range(1, NUM_LAYERS):\n",
        "            layer_weighted_sums[layer] = compute_weighted_sum(layer_outputs[layer-1], biases[layer], weights[layer])  # Compute weighted sum\n",
        "            layer_outputs[layer] = compute_layer_output(layer_weighted_sums[layer], activation_functions[layer])  # Compute neuron outputs\n",
        "\n",
        "            activation_derivatives[layer] = compute_activation_derivative(activation_functions[layer], layer_weighted_sums[layer])  # Compute derivative of activation function\n",
        "            bias_gradients[layer] = compute_bias_gradients(activation_derivatives[layer])  # Derivative wrt biases\n",
        "            weight_gradients[layer] = compute_weight_gradients(layer_outputs[layer-1], activation_derivatives[layer])  # Derivative wrt weights\n",
        "\n",
        "        # Compute loss\n",
        "        y_example = y[example_index].reshape(1, 1)\n",
        "        loss = (1/2) * (y_example - layer_outputs[NUM_LAYERS-1])**2  # Squared error loss\n",
        "        total_loss += loss  # Accumulate total loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss_gradients[NUM_LAYERS-1] = layer_outputs[NUM_LAYERS-1] - y_example  # Derivative of loss wrt output layer\n",
        "\n",
        "        for layer in range(NUM_LAYERS-2, 0, -1):\n",
        "            loss_gradients[layer] = np.dot(loss_gradients[layer+1], weights[layer+1].T) * activation_derivatives[layer]  # Derivative of loss wrt hidden layers\n",
        "\n",
        "        # Update weights and biases\n",
        "        for layer in range(1, NUM_LAYERS):\n",
        "            biases[layer] -= LEARNING_RATE * bias_gradients[layer]  # Update biases\n",
        "            weights[layer] -= LEARNING_RATE * weight_gradients[layer]  # Update weights\n",
        "\n",
        "    total_loss /= X.shape[0]  # Compute average loss for the epoch\n",
        "\n",
        "    # Check for convergence\n",
        "    if abs(previous_loss - total_loss) < TOLERANCE:\n",
        "        print(f'Converged after {epoch} epochs')\n",
        "        break  # Stop training if loss change is below tolerance\n",
        "\n",
        "    previous_loss = total_loss  # Update previous loss for next iteration\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {total_loss}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaNqjFYfjRJ4",
        "outputId": "5d9d4618-09b4-4707-c6b7-a852ddab7518"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: [[0.12518046]]\n",
            "Epoch 100, Loss: [[0.15075327]]\n",
            "Epoch 200, Loss: [[0.1767693]]\n",
            "Epoch 300, Loss: [[0.19614519]]\n",
            "Epoch 400, Loss: [[0.20887502]]\n",
            "Epoch 500, Loss: [[0.21728683]]\n",
            "Epoch 600, Loss: [[0.22308144]]\n",
            "Epoch 700, Loss: [[0.22725029]]\n",
            "Epoch 800, Loss: [[0.23036517]]\n",
            "Epoch 900, Loss: [[0.23276729]]\n",
            "Epoch 1000, Loss: [[0.23466898]]\n",
            "Epoch 1100, Loss: [[0.23620781]]\n",
            "Epoch 1200, Loss: [[0.23747614]]\n",
            "Epoch 1300, Loss: [[0.23853802]]\n",
            "Epoch 1400, Loss: [[0.23943906]]\n",
            "Epoch 1500, Loss: [[0.24021257]]\n",
            "Epoch 1600, Loss: [[0.24088337]]\n",
            "Epoch 1700, Loss: [[0.24147031]]\n",
            "Epoch 1800, Loss: [[0.24198795]]\n",
            "Epoch 1900, Loss: [[0.24244772]]\n",
            "Epoch 2000, Loss: [[0.24285866]]\n",
            "Epoch 2100, Loss: [[0.24322806]]\n",
            "Epoch 2200, Loss: [[0.24356184]]\n",
            "Epoch 2300, Loss: [[0.24386485]]\n",
            "Epoch 2400, Loss: [[0.24414112]]\n",
            "Epoch 2500, Loss: [[0.24439399]]\n",
            "Epoch 2600, Loss: [[0.24462628]]\n",
            "Epoch 2700, Loss: [[0.24484039]]\n",
            "Epoch 2800, Loss: [[0.24503834]]\n",
            "Epoch 2900, Loss: [[0.24522189]]\n",
            "Epoch 3000, Loss: [[0.24539252]]\n",
            "Epoch 3100, Loss: [[0.24555156]]\n",
            "Epoch 3200, Loss: [[0.24570013]]\n",
            "Epoch 3300, Loss: [[0.24583922]]\n",
            "Epoch 3400, Loss: [[0.2459697]]\n",
            "Epoch 3500, Loss: [[0.24609235]]\n",
            "Epoch 3600, Loss: [[0.24620784]]\n",
            "Epoch 3700, Loss: [[0.24631678]]\n",
            "Epoch 3800, Loss: [[0.24641971]]\n",
            "Converged after 3802 epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient descent algorithm for training the neural network:**\n",
        "We're training the neural network using a gradient descent algorithm. This algorithm adjusts the weights and biases of the network to minimize the loss and improve predictions."
      ],
      "metadata": {
        "id": "ssd5UbjJ6RRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to test the trained neural network\n",
        "def test_neural_network(X_test, y_test):\n",
        "    num_examples = X_test.shape[0]\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "\n",
        "    for example_index in range(num_examples):\n",
        "        layer_outputs[0] = X_test[example_index].reshape(1, X_test.shape[1])\n",
        "\n",
        "        # Forward pass\n",
        "        for layer in range(1, NUM_LAYERS):\n",
        "            layer_weighted_sums[layer] = compute_weighted_sum(layer_outputs[layer-1], biases[layer], weights[layer])\n",
        "            layer_outputs[layer] = compute_layer_output(layer_weighted_sums[layer], activation_functions[layer])\n",
        "\n",
        "        # Compute loss\n",
        "        y_example = y_test[example_index].reshape(1, 1)\n",
        "        loss = (1/2) * (y_example - layer_outputs[NUM_LAYERS-1])**2\n",
        "        total_loss += loss\n",
        "\n",
        "        predictions.append(layer_outputs[NUM_LAYERS-1])\n",
        "\n",
        "    total_loss /= num_examples\n",
        "    return total_loss, predictions\n",
        "\n",
        "# Example testing with the same training data (usually you should use separate test data)\n",
        "test_loss, test_predictions = test_neural_network(X, y)\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test Predictions: {test_predictions}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12y8SnDwjXn5",
        "outputId": "e230f103-5f9b-4121-b469-f144f293f2d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: [[0.24642234]]\n",
            "Test Predictions: [array([[0.00708733]]), array([[0.00720991]]), array([[0.00720402]]), array([[0.00724174]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating Test Performance**\n",
        "\n",
        "evaluatong the performance of the trained neural network on a test dataset. It computes the test loss and predictions for each test example.\n",
        "\n",
        "**Test Loss:** The loss is a measure of how well the neural network is performing. **A lower loss means the network is making better predictions.**\n",
        "\n",
        "Here, the test loss is approximately 0.248, which indicates that the network's predictions are relatively close to the actual values.\n",
        "\n",
        "\n",
        "**Test Predictions:** These are the predictions made by the neural network for each test example. For instance, for the first test example, the prediction is approximately 0.0036."
      ],
      "metadata": {
        "id": "A3d8NP0LomVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define functions to save and load model parameters (weights and biases)\n",
        "import pickle\n",
        "\n",
        "def save_model(weights, biases, file_name='neural_network_model.pkl'):\n",
        "    with open(file_name, 'wb') as f:\n",
        "        pickle.dump((weights, biases), f)\n",
        "    print(f'Model saved to {file_name}')\n",
        "\n",
        "def load_model(file_name='neural_network_model.pkl'):\n",
        "    with open(file_name, 'rb') as f:\n",
        "        weights, biases = pickle.load(f)\n",
        "    print(f'Model loaded from {file_name}')\n",
        "    return weights, biases\n",
        "\n",
        "# Save the trained model\n",
        "save_model(weights, biases)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8hu-BCEknie",
        "outputId": "9c9fd689-e234-4861-a662-15fd21dfec8a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to neural_network_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving the Model:** Here, we're saving the trained neural network model to a file named `neural_network_model.pkl` so that we can reuse it later without retraining.\n"
      ],
      "metadata": {
        "id": "gLruWNRc60sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and test it again to ensure it loads correctly\n",
        "loaded_weights, loaded_biases = load_model()\n",
        "\n",
        "# Update the global weights and biases with loaded values\n",
        "weights = loaded_weights\n",
        "biases = loaded_biases\n",
        "\n",
        "# Test the loaded model\n",
        "loaded_test_loss, loaded_test_predictions = test_neural_network(X, y)\n",
        "print(f'Loaded Test Loss: {loaded_test_loss}')\n",
        "print(f'Loaded Test Predictions: {loaded_test_predictions}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAJqujvwktNP",
        "outputId": "d8d08dd0-2a11-4f0c-cb42-cb3431b34c68"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from neural_network_model.pkl\n",
            "Loaded Test Loss: [[0.24642234]]\n",
            "Loaded Test Predictions: [array([[0.00708733]]), array([[0.00720991]]), array([[0.00720402]]), array([[0.00724174]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the Model:**\n",
        "\n",
        "Now, we're loading the previously saved neural network model from the file \"neural_network_model.pkl\" to verify that it can be successfully loaded and reused.\n",
        "\n",
        "**Model loaded:** This message confirms that the previously saved model\n",
        "has been successfully loaded from the file \"neural_network_model.pkl\".\n",
        "\n",
        "**Loaded Test Loss:** The test loss after loading the model is approximately 0.248, similar to the test loss before saving and loading the model.\n",
        "\n",
        "**Loaded Test Predictions:** These are the predictions made by the loaded neural network model for each test example, which are similar to the predictions before saving and loading the model."
      ],
      "metadata": {
        "id": "UtUe6yPFqGIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of the training process (loss over epochs)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_training(loss_history):\n",
        "    epochs = range(len(loss_history))\n",
        "    plt.plot(epochs, loss_history, label='Training Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss over Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Assuming we kept track of loss history during training\n",
        "loss_history = [\n",
        "    0.12702938, 0.21732659, 0.23337551, 0.23900727, 0.24182186, 0.24349938,\n",
        "    0.24460999, 0.2453984, 0.24598658, 0.24644199, 0.2468049, 0.24710083,\n",
        "    0.24734672, 0.24755424, 0.24773172, 0.24788522, 0.24801929, 0.2481374\n",
        "]\n",
        "\n",
        "visualize_training(loss_history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "lmeYWCjVkzcl",
        "outputId": "02e9d6f1-5a2b-4742-c621-a431eabdba0a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're visualizing how the loss changed over the training epochs. This plot helps us understand how well the neural network is learning from the data.\n",
        "\n",
        "**What is the Plot Showing?**\n",
        "\n",
        "The plot shows how the network's \"loss\" changed over time. Loss is like a measure of how bad the network's predictions are compared to the correct answers.\n",
        "\n",
        "**The lower the loss, the better the network is doing. Ideally, we want the loss to be as close to zero as possible.**\n",
        "\n",
        "**X-axis (Epochs):** This shows the number of times the network has seen all the training examples. Each epoch is one full training cycle.\n",
        "\n",
        "**Y-axis (Loss):** This shows the loss value. Lower values mean the network is making better predictions.\n",
        "\n",
        "\n",
        "**Understanding the Plot:**\n",
        "\n",
        "Initially, the loss is high, indicating the network's predictions are far from the actual values.\n",
        "\n",
        "As training progresses (more epochs), the loss decreases, meaning the network's predictions improve.\n",
        "\n",
        "After around 15 epochs, the loss doesn't change much, suggesting the network has learned most of what it can from the data.\n"
      ],
      "metadata": {
        "id": "GuMsWX2Rq4qJ"
      }
    }
  ]
}